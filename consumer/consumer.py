from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'bigdata',
    bootstrap_servers='kafka:9092',
    auto_offset_reset='latest',
    enable_auto_commit=True,
    group_id='data-logger-group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
    key_deserializer=lambda k: k.decode('utf-8') if k else None
)

print("ğŸš€ Consumer Ä‘Ã£ káº¿t ná»‘i, Ä‘ang theo dÃµi tÄƒng tráº§n / giáº£m sÃ n...\n")

for message in consumer:
    symbol = message.key
    data = message.value
    price = data['price']
    high = data['high']
    low = data['low']
    volume = data ['volume']

    print(f"[{symbol}] GiÃ¡ hiá»‡n táº¡i: {price} | Cao nháº¥t: {high} | Tháº¥p nháº¥t: {low}")

    # Kiá»ƒm tra tÄƒng tráº§n / giáº£m sÃ n
    if price >= high:
        print(f"ğŸš€ğŸš€ {symbol} Ä‘ang TÄ‚NG TRáº¦N!")
    elif price <= low:
        print(f"ğŸ”»ğŸ”» {symbol} Ä‘ang GIáº¢M SÃ€N!")
    else: print("KhÃ´ng cÃ³ báº¥t thÆ°á»ng")



# from pyspark.sql import SparkSession
# from pyspark.sql.functions import from_json, col, expr, when, lag
# from pyspark.sql.types import StructType, StringType, FloatType, TimestampType
# from pyspark.sql.window import Window

# spark = SparkSession.builder \
#     .appName("KafkaSparkStreamingAnalysis") \
#     .config("spark.jars.packages", 
#             "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \
#     .getOrCreate()

# spark.sparkContext.setLogLevel("WARN")

# # Schema má»Ÿ rá»™ng Ä‘á»ƒ chá»©a nhiá»u thÃ´ng tin hÆ¡n
# schema = StructType() \
#     .add("symbol", StringType()) \
#     .add("price", FloatType()) \
#     .add("high", FloatType()) \
#     .add("low", FloatType()) \
#     .add("open", FloatType()) \
#     .add("prev_close", FloatType()) \
#     .add("timestamp", StringType())  # sáº½ Ã©p kiá»ƒu sang Timestamp sau

# # Äá»c tá»« Kafka
# df_raw = spark.readStream \
#     .format("kafka") \
#     .option("kafka.bootstrap.servers", "kafka:9092") \
#     .option("subscribe", "bigdata") \
#     .option("startingOffsets", "latest") \
#     .load()

# # Parse JSON
# df_parsed = df_raw.selectExpr("CAST(value AS STRING) as json_str") \
#     .select(from_json(col("json_str"), schema).alias("data")) \
#     .select("data.*") \
#     .withColumn("event_time", expr("CAST(timestamp AS TIMESTAMP)"))

# # TÃ­nh toÃ¡n giÃ¡ thay Ä‘á»•i so vá»›i giÃ¡ Ä‘Ã³ng cá»­a trÆ°á»›c
# df_analyzed = df_parsed.withColumn(
#     "price_change", col("price") - col("prev_close")
# ).withColumn(
#     "change_percent", ((col("price") - col("prev_close")) / col("prev_close")) * 100
# )

# # Cá»­a sá»• thá»i gian theo symbol
# window_spec = Window.partitionBy("symbol").orderBy("event_time")

# # Dá»¯ liá»‡u giÃ¡ trÆ°á»›c Ä‘Ã³ (so sÃ¡nh Ä‘á»ƒ xÃ¡c Ä‘á»‹nh Ä‘á»™t biáº¿n)
# df_with_lag = df_analyzed.withColumn("prev_price", lag("price").over(window_spec)) \
#     .withColumn("price_jump_percent", ((col("price") - col("prev_price")) / col("prev_price")) * 100)

# # ÄÃ¡nh dáº¥u cÃ¡c hiá»‡n tÆ°á»£ng báº¥t thÆ°á»ng
# df_flagged = df_with_lag.withColumn(
#     "is_spike", when(col("price_jump_percent") > 5, "ğŸš€ TÄƒng Ä‘á»™t biáº¿n").when(col("price_jump_percent") < -5, "ğŸ“‰ Giáº£m Ä‘á»™t biáº¿n")
# ).withColumn(
#     "is_limit_up", when(col("change_percent") >= 6.9, "â¬† TÄƒng tráº§n")  # TÃ¹y sÃ n, thÆ°á»ng lÃ  ~7%
# ).withColumn(
#     "is_limit_down", when(col("change_percent") <= -6.9, "â¬‡ Giáº£m sÃ n")
# )

# # Hiá»ƒn thá»‹ ra console
# query = df_flagged.select("symbol", "price", "change_percent", "is_spike", "is_limit_up", "is_limit_down", "event_time") \
#     .writeStream \
#     .outputMode("append") \
#     .format("console") \
#     .option("truncate", False) \
#     .start()

# query.awaitTermination()